#!/usr/bin/env python3
"""
Unified model management for the model serving stack.

Wraps Ollama commands and automatically syncs to LM Studio + regenerates config.

Usage:
    ./model start                  Start the server stack (runs in background)
    ./model stop                   Stop all services
    ./model status                 Check service status (processes running)
    ./model stats                  Show loaded models with resource usage
    ./model pull <model-name>      Pull a model and sync
    ./model rm <model-name>        Remove a model and sync
    ./model list [filter]          List Ollama models (optional filter)
    ./model list --config [filter] List llama-swap model names from config
    ./model names [filter]         Show Ollama â†’ llama-swap name mapping
    ./model show <filter>          Show full config for matching models
    ./model sync                   Just sync and regenerate config
    ./model run <model-name>       Run a model interactively (passthrough to ollama)

Examples:
    ./model start                  # Start server, logs to model_serve.log
    ./model stats                  # See what's loaded and memory usage
    ./model pull gemma3:27b        # Pull and auto-sync
    ./model list derestricted      # Filter Ollama models by name
    ./model list --config gemma    # Show llama-swap names matching 'gemma'
    ./model names qwen             # Show name mapping for qwen models
    ./model show gemma3:12         # Show full config for matching models
"""

import os
import sys
import subprocess
from pathlib import Path


SCRIPT_DIR = Path(__file__).parent.resolve()


def load_env():
    """Load .env file if it exists."""
    env_file = SCRIPT_DIR / '.env'
    if env_file.exists():
        with open(env_file) as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    os.environ.setdefault(key.strip(), value.strip())


def run_cmd(cmd: list[str], check: bool = True) -> subprocess.CompletedProcess:
    """Run a command and return the result."""
    print(f"$ {' '.join(cmd)}")
    return subprocess.run(cmd, check=check)


def ollama_pull(model_name: str) -> bool:
    """Pull a model using Ollama."""
    print(f"\n=== Pulling {model_name} via Ollama ===\n")
    result = run_cmd(['ollama', 'pull', model_name], check=False)
    return result.returncode == 0


def ollama_rm(model_name: str) -> bool:
    """Remove a model using Ollama."""
    print(f"\n=== Removing {model_name} via Ollama ===\n")
    result = run_cmd(['ollama', 'rm', model_name], check=False)
    return result.returncode == 0


def ollama_list(filter_term: str = None) -> bool:
    """List models in Ollama, optionally filtered."""
    if filter_term:
        # Get output and filter it
        result = subprocess.run(
            ['ollama', 'list'],
            capture_output=True,
            text=True,
            check=False
        )
        if result.returncode != 0:
            print(result.stderr)
            return False

        lines = result.stdout.strip().split('\n')
        filter_lower = filter_term.lower()

        # Print header
        if lines:
            print(lines[0])

        # Filter and print matching lines
        count = 0
        for line in lines[1:]:
            if filter_lower in line.lower():
                print(line)
                count += 1

        print(f"\n({count} models matching '{filter_term}')")
        return True
    else:
        result = subprocess.run(['ollama', 'list'], check=False)
        return result.returncode == 0


def ollama_run(model_name: str) -> bool:
    """Run a model interactively."""
    print(f"\n=== Running {model_name} ===\n")
    result = run_cmd(['ollama', 'run', model_name], check=False)
    return result.returncode == 0


def sync_bridge() -> bool:
    """Run the Ollama-LM Studio bridge sync."""
    bridge_script = os.environ.get('BRIDGE_SCRIPT', '')

    # Expand ~ to home directory
    if bridge_script.startswith('~'):
        bridge_script = os.path.expanduser(bridge_script)

    # Default to bin/ if not set
    if not bridge_script:
        bridge_script = str(SCRIPT_DIR / 'bin' / 'lm-studio-ollama-bridge')

    print(f"\n=== Syncing to LM Studio ===\n")

    if not os.path.exists(bridge_script):
        print(f"Warning: Bridge not found: {bridge_script}")
        print("Run ./install.sh to build it")
        return False

    result = run_cmd([bridge_script], check=False)
    return result.returncode == 0


def find_python_with_yaml() -> str:
    """Find a Python interpreter that has PyYAML installed."""
    candidates = [
        '/opt/miniconda3/bin/python3',  # Conda
        '/opt/homebrew/bin/python3',     # Homebrew Apple Silicon
        '/usr/local/bin/python3',        # Homebrew Intel / Linux
        'python3',                        # Whatever's in PATH
        sys.executable,                   # Current interpreter
    ]

    for python in candidates:
        try:
            result = subprocess.run(
                [python, '-c', 'import yaml'],
                capture_output=True,
                timeout=5
            )
            if result.returncode == 0:
                return python
        except (FileNotFoundError, subprocess.TimeoutExpired):
            continue

    # Fallback to current interpreter
    return sys.executable


def regenerate_config() -> bool:
    """Regenerate llama-swap config."""
    print(f"\n=== Regenerating config ===\n")

    python = find_python_with_yaml()
    generate_script = SCRIPT_DIR / 'generate_config.py'
    result = run_cmd([python, str(generate_script)], check=False)
    return result.returncode == 0


def reload_llama_swap() -> bool:
    """Restart llama-swap to pick up config changes."""
    import signal

    port = os.environ.get('LLAMA_SWAP_PORT', '5847')

    # Find llama-swap process
    try:
        result = subprocess.run(
            ['pgrep', '-f', 'llama-swap.*--config'],
            capture_output=True,
            text=True
        )
        if result.returncode != 0:
            print("llama-swap not running (this is OK)")
            return False

        pids = result.stdout.strip().split('\n')
        if not pids or not pids[0]:
            print("llama-swap not running (this is OK)")
            return False

        # Kill existing llama-swap
        for pid in pids:
            if pid:
                os.kill(int(pid), signal.SIGTERM)
        print("Stopped llama-swap")

        # Wait for it to die
        import time
        time.sleep(1)

        # Find llama-swap binary
        llama_swap_bin = None
        for path in ['/opt/homebrew/bin/llama-swap', '/usr/local/bin/llama-swap',
                     str(SCRIPT_DIR / 'bin' / 'llama-swap')]:
            if os.path.exists(path):
                llama_swap_bin = path
                break

        if not llama_swap_bin:
            # Try which
            result = subprocess.run(['which', 'llama-swap'], capture_output=True, text=True)
            if result.returncode == 0:
                llama_swap_bin = result.stdout.strip()

        if not llama_swap_bin:
            print("Warning: llama-swap binary not found, cannot restart")
            return False

        # Restart llama-swap
        config_path = SCRIPT_DIR / 'config.yaml'
        subprocess.Popen(
            [llama_swap_bin, '--config', str(config_path), '--listen', f'0.0.0.0:{port}'],
            stdout=subprocess.DEVNULL,
            stderr=subprocess.DEVNULL,
            start_new_session=True
        )
        print(f"Restarted llama-swap on port {port}")
        return True

    except Exception as e:
        print(f"Failed to reload llama-swap: {e}")
        return False


def cleanup_broken_symlinks() -> int:
    """Remove broken symlinks from LM Studio models directory."""
    models_dir = os.environ.get('MODELS_DIR', '~/.cache/lm-studio/models')
    models_dir = os.path.expanduser(models_dir)

    if not os.path.exists(models_dir):
        return 0

    print(f"\n=== Cleaning up broken symlinks ===\n")

    broken_count = 0
    for root, dirs, files in os.walk(models_dir):
        for name in files:
            path = os.path.join(root, name)
            if os.path.islink(path) and not os.path.exists(path):
                os.remove(path)
                broken_count += 1

    # Remove empty directories
    for root, dirs, files in os.walk(models_dir, topdown=False):
        for name in dirs:
            dir_path = os.path.join(root, name)
            try:
                if not os.listdir(dir_path):
                    os.rmdir(dir_path)
            except OSError:
                pass

    if broken_count > 0:
        print(f"Removed {broken_count} broken symlinks")
    else:
        print("No broken symlinks found")

    return broken_count


def get_config_hash() -> str:
    """Get hash of current config.yaml for change detection."""
    import hashlib
    config_path = SCRIPT_DIR / 'config.yaml'
    if not config_path.exists():
        return ""
    return hashlib.md5(config_path.read_bytes()).hexdigest()


def do_sync():
    """Full sync: cleanup + bridge + regenerate + reload if changed."""
    old_hash = get_config_hash()

    cleanup_broken_symlinks()
    sync_bridge()
    regenerate_config()

    new_hash = get_config_hash()
    if new_hash != old_hash:
        print("Config changed, reloading llama-swap...")
        reload_llama_swap()
    else:
        print("Config unchanged, skipping reload")


def is_mxfp4_model(model_name: str) -> bool:
    """Check if model name contains MXFP4 (case-insensitive)."""
    return "mxfp4" in model_name.lower()


def extract_ollama_slug(ollama_model_name: str) -> str:
    """
    Extract the model slug from an Ollama model name.

    Examples:
        hf.co/Felladrin/gguf-MXFP4-gpt-oss-20b-Derestricted:latest
        -> gguf-mxfp4-gpt-oss-20b-derestricted

        hf.co/gghfez/gpt-oss-120b-Derestricted.MXFP4_MOE-gguf:latest
        -> gpt-oss-120b-derestricted.mxfp4_moe-gguf
    """
    name = ollama_model_name.lower()

    # Remove hf.co/<user>/ prefix if present
    if name.startswith('hf.co/'):
        parts = name.split('/', 2)
        if len(parts) >= 3:
            name = parts[2]

    # Remove :tag suffix if present
    if ':' in name:
        name = name.split(':')[0]

    return name


def update_mxfp4_configs(ollama_model_name: str):
    """
    Update harmony_models.yaml and custom_models.yaml for MXFP4 models.

    After sync, finds the generated ls/ name by exact matching on the model slug,
    then adds:
    - Entry to harmony_models.yaml (if not present)
    - Entry to custom_models.yaml with ollama_model mapping (if not present)
    """
    import yaml

    # Load generated config to find the ls/ name for this model
    config = load_config()
    models = config.get('models', {})

    # Load existing custom_models to check which already have mappings
    custom_path = SCRIPT_DIR / 'custom_models.yaml'
    custom_config = {}
    if custom_path.exists():
        with open(custom_path) as f:
            custom_config = yaml.safe_load(f) or {}
    custom_models = custom_config.get('models', {})

    # Extract the slug from the Ollama model name for exact matching
    # e.g., "hf.co/Felladrin/gguf-MXFP4-gpt-oss-20b-Derestricted:latest"
    #    -> "gguf-mxfp4-gpt-oss-20b-derestricted"
    ollama_slug = extract_ollama_slug(ollama_model_name)

    # Find the ls/ name that contains this exact slug
    ls_name = None
    for name in models.keys():
        if not is_mxfp4_model(name):
            continue

        # Strip ls/ prefix for the base name
        base = name[3:] if name.startswith('ls/') else name

        # Check if the config entry contains the Ollama slug (exact match)
        # The config name is: slug + size suffix (e.g., "-20.9b-latest")
        if ollama_slug in base.lower():
            ls_name = base
            break

    if not ls_name:
        print(f"Note: No config entry found matching slug '{ollama_slug}'")
        print(f"  (Ollama model: {ollama_model_name})")
        return

    print(f"\n=== Updating MXFP4 configs for {ls_name} ===\n")

    # Update harmony_models.yaml
    harmony_path = SCRIPT_DIR / 'harmony_models.yaml'
    harmony_config = {}
    if harmony_path.exists():
        with open(harmony_path) as f:
            harmony_config = yaml.safe_load(f) or {}

    harmony_models = harmony_config.get('harmony_models', [])
    prefixed_name = f"ls/{ls_name}"
    if prefixed_name not in harmony_models:
        harmony_models.append(prefixed_name)
        harmony_config['harmony_models'] = harmony_models
        with open(harmony_path, 'w') as f:
            yaml.dump(harmony_config, f, default_flow_style=False)
        print(f"Added {prefixed_name} to harmony_models.yaml")
    else:
        print(f"{prefixed_name} already in harmony_models.yaml")

    # Update custom_models.yaml
    if ls_name not in custom_models:
        custom_models[ls_name] = {}

    if 'ollama_model' not in custom_models[ls_name]:
        custom_models[ls_name]['ollama_model'] = ollama_model_name
        # Add default settings for GPT-OSS/Harmony models
        custom_models[ls_name].setdefault('sampler_args', '--temp 0.7 --top-p 0.95 --top-k 40')
        custom_models[ls_name].setdefault('ctx_size', 65536)
        custom_models[ls_name].setdefault('jinja', False)

        custom_config['models'] = custom_models
        with open(custom_path, 'w') as f:
            yaml.dump(custom_config, f, default_flow_style=False, sort_keys=False)
        print(f"Added {ls_name} with ollama_model mapping to custom_models.yaml")
    else:
        print(f"{ls_name} already has ollama_model mapping in custom_models.yaml")


def cmd_pull(args: list[str]):
    """Handle pull command."""
    if not args:
        print("Usage: model pull <model-name>")
        print("Example: model pull gemma3:12b")
        sys.exit(1)

    model_name = args[0]

    if not ollama_pull(model_name):
        print(f"\nFailed to pull {model_name}")
        sys.exit(1)

    do_sync()

    # For MXFP4 models, update harmony_models.yaml and custom_models.yaml
    if is_mxfp4_model(model_name):
        update_mxfp4_configs(model_name)

    print(f"\n=== Done! {model_name} is ready ===")


def cmd_rm(args: list[str]):
    """Handle rm command."""
    if not args:
        print("Usage: model rm <model-name>")
        print("Example: model rm gemma3:12b")
        sys.exit(1)

    model_name = args[0]

    if not ollama_rm(model_name):
        print(f"\nFailed to remove {model_name}")
        sys.exit(1)

    do_sync()
    print(f"\n=== Done! {model_name} removed ===")


def load_config() -> dict:
    """Load config.yaml and return as dict."""
    import yaml
    config_path = SCRIPT_DIR / 'config.yaml'
    if not config_path.exists():
        return {}
    with open(config_path) as f:
        return yaml.safe_load(f) or {}


def cmd_list(args: list[str]):
    """Handle list command with optional filter."""
    # Check for --config flag
    if args and args[0] == '--config':
        filter_term = args[1] if len(args) > 1 else None
        cmd_list_config(filter_term)
        return

    filter_term = args[0] if args else None
    ollama_list(filter_term)


def cmd_list_config(filter_term: str = None):
    """List model names from config.yaml (llama-swap names)."""
    config = load_config()
    models = config.get('models', {})

    if not models:
        print("No models in config.yaml. Run './model sync' first.")
        return

    print(f"{'MODEL NAME':<60} {'CTX':>8}")
    print("=" * 70)

    count = 0
    for model_name, model_config in sorted(models.items()):
        if filter_term and filter_term.lower() not in model_name.lower():
            continue

        # Extract ctx_size from cmd
        cmd = model_config.get('cmd', '')
        ctx_size = '-'
        if '--ctx-size' in cmd:
            try:
                parts = cmd.split('--ctx-size')[1].split()
                ctx_size = f"{int(parts[0]):,}"
            except:
                pass

        print(f"{model_name:<60} {ctx_size:>8}")
        count += 1

    print(f"\n({count} models" + (f" matching '{filter_term}'" if filter_term else "") + ")")


def cmd_names(args: list[str]):
    """Show llama-swap model names (what to use in API requests)."""
    config = load_config()
    models = config.get('models', {})
    filter_term = args[0] if args else None

    if not models:
        print("No models in config.yaml. Run './model sync' first.")
        return

    prefix = os.environ.get('MODEL_PREFIX', 'ls/')

    # Group models by family (first part of name before colon or dash)
    families = {}
    for llama_swap_name, model_config in models.items():
        # Remove prefix
        if prefix and llama_swap_name.startswith(prefix):
            base_name = llama_swap_name[len(prefix):]
        else:
            base_name = llama_swap_name

        # Apply filter
        if filter_term and filter_term.lower() not in llama_swap_name.lower():
            continue

        # Get family name (before first colon)
        family = base_name.split(':')[0] if ':' in base_name else base_name.split('-')[0]

        if family not in families:
            families[family] = []
        families[family].append((llama_swap_name, model_config))

    if not families:
        print(f"No models matching '{filter_term}'")
        return

    count = 0
    for family in sorted(families.keys()):
        print(f"\n{family}:")
        for llama_swap_name, model_config in sorted(families[family]):
            description = model_config.get('description', '')

            # Show alias info if present
            if description.startswith('alias of '):
                alias_target = description.split('|')[0].replace('alias of ', '').strip()
                print(f"  {llama_swap_name:<50} (alias of {alias_target})")
            else:
                # Extract size from name for display
                print(f"  {llama_swap_name}")
            count += 1

    print(f"\n({count} models" + (f" matching '{filter_term}'" if filter_term else "") + ")")
    print("\nUse these names in API requests and custom_models.yaml (without ls/ prefix for yaml)")


def cmd_show(args: list[str]):
    """Show full configuration for matching models."""
    if not args:
        print("Usage: model show <filter>")
        print("Example: model show gemma3:12")
        sys.exit(1)

    filter_term = args[0].lower()
    config = load_config()
    models = config.get('models', {})

    if not models:
        print("No models in config.yaml. Run './model sync' first.")
        return

    matches = []
    for model_name, model_config in models.items():
        if filter_term in model_name.lower():
            matches.append((model_name, model_config))

    if not matches:
        print(f"No models matching '{args[0]}'")
        return

    for model_name, model_config in sorted(matches):
        print(f"\n{'='*70}")
        print(f"MODEL: {model_name}")
        print(f"{'='*70}")

        cmd = model_config.get('cmd', '')
        proxy = model_config.get('proxy', '')
        ttl = model_config.get('ttl', '')
        description = model_config.get('description', '')

        # Parse settings from cmd
        print(f"\nSettings:")

        # ctx_size
        if '--ctx-size' in cmd:
            try:
                ctx = cmd.split('--ctx-size')[1].split()[0]
                print(f"  ctx_size:    {int(ctx):,}")
            except:
                pass

        # parallel
        if '--parallel' in cmd:
            try:
                parallel = cmd.split('--parallel')[1].split()[0]
                print(f"  parallel:    {parallel}")
            except:
                pass

        # Sampler args
        for arg in ['--temp', '--top-nsigma', '--min-p', '--top-p', '--top-k']:
            if arg in cmd:
                try:
                    val = cmd.split(arg)[1].split()[0]
                    print(f"  {arg.lstrip('-'):<12} {val}")
                except:
                    pass

        print(f"  ttl:         {ttl}s")

        if description:
            print(f"\nDescription: {description}")

        print(f"\nFull command:")
        # Word wrap the command for readability
        print(f"  {cmd}")

    print(f"\n({len(matches)} model(s) matching '{args[0]}')")


def cmd_sync(args: list[str]):
    """Handle sync command."""
    do_sync()
    print("\n=== Sync complete ===")


def cmd_run(args: list[str]):
    """Handle run command (passthrough to ollama)."""
    if not args:
        print("Usage: model run <model-name>")
        sys.exit(1)

    ollama_run(args[0])


def cmd_stats(args: list[str]):
    """Show running models with resource usage."""
    import requests

    port = os.environ.get('LLAMA_SWAP_PORT', '5847')
    api_url = f"http://127.0.0.1:{port}"

    # Get running models from llama-swap
    try:
        response = requests.get(f"{api_url}/running", timeout=5)
        response.raise_for_status()
        data = response.json()
        running = data.get('running', [])
    except requests.exceptions.RequestException as e:
        print(f"Error: Cannot connect to llama-swap at {api_url}")
        print(f"  {e}")
        sys.exit(1)

    if not running:
        print("No models currently loaded")
        return

    print(f"\n{'='*70}")
    print(f"{'MODEL':<45} {'CTX':>8} {'SLOTS':>6} {'STATE':>8}")
    print(f"{'='*70}")

    total_ctx = 0
    total_slots = 0

    for model_info in running:
        model_name = model_info.get('model', 'unknown')
        proxy_url = model_info.get('proxy', '')
        state = model_info.get('state', 'unknown')

        # Query the model's llama-server for detailed stats
        ctx_size = 0
        num_slots = 0

        if proxy_url:
            try:
                # Get slot info
                slots_resp = requests.get(f"{proxy_url}/slots", timeout=2)
                if slots_resp.ok:
                    slots = slots_resp.json()
                    num_slots = len(slots)
                    if slots:
                        ctx_size = slots[0].get('n_ctx', 0)
            except:
                # Try to parse from cmd if /slots fails
                cmd = model_info.get('cmd', '')
                if '--ctx-size' in cmd:
                    try:
                        parts = cmd.split('--ctx-size')[1].split()
                        ctx_size = int(parts[0])
                    except:
                        pass

        total_ctx += ctx_size
        total_slots += num_slots

        # Truncate long model names
        display_name = model_name[:44] if len(model_name) > 44 else model_name
        ctx_display = f"{ctx_size:,}" if ctx_size else "-"

        print(f"{display_name:<45} {ctx_display:>8} {num_slots:>6} {state:>8}")

    print(f"{'='*70}")
    print(f"{'TOTAL':<45} {total_ctx:>8,} {total_slots:>6}")
    print()

    # System memory
    try:
        import psutil
        mem = psutil.virtual_memory()
        print(f"System Memory: {mem.percent}% used ({mem.used/1024**3:.1f}GB / {mem.total/1024**3:.1f}GB)")
    except ImportError:
        pass


def cmd_start(args: list[str]):
    """Start the server stack in background."""
    log_file = SCRIPT_DIR / 'model_serve.log'

    # Check if already running
    result = subprocess.run(['pgrep', '-f', 'llama-swap.*--config'], capture_output=True)
    if result.returncode == 0:
        print("Server already running. Use './model stop' first or './model status' to check.")
        return

    print(f"Starting server stack...")
    print(f"Logs: {log_file}")

    # Start in background with output to log file
    with open(log_file, 'a') as f:
        subprocess.Popen(
            [str(SCRIPT_DIR / 'start.sh')],
            stdout=f,
            stderr=subprocess.STDOUT,
            start_new_session=True
        )

    print("Server starting in background. Use './model status' to check.")


def cmd_stop(args: list[str]):
    """Stop all services."""
    print("Stopping server stack...")
    result = subprocess.run([str(SCRIPT_DIR / 'stop.sh')], check=False)
    if result.returncode == 0:
        print("Server stopped.")
    else:
        print("Stop may have encountered issues. Check './model status'.")


def cmd_status(args: list[str]):
    """Check service status."""
    subprocess.run([str(SCRIPT_DIR / 'status.sh')], check=False)


def cmd_help(args: list[str]):
    """Show help."""
    print(__doc__)


def main():
    load_env()

    commands = {
        'start': cmd_start,
        'stop': cmd_stop,
        'status': cmd_status,
        'pull': cmd_pull,
        'rm': cmd_rm,
        'remove': cmd_rm,
        'list': cmd_list,
        'ls': cmd_list,
        'names': cmd_names,
        'show': cmd_show,
        'sync': cmd_sync,
        'run': cmd_run,
        'stats': cmd_stats,
        'help': cmd_help,
        '--help': cmd_help,
        '-h': cmd_help,
    }

    if len(sys.argv) < 2:
        cmd_help([])
        sys.exit(1)

    cmd = sys.argv[1].lower()
    args = sys.argv[2:]

    if cmd in commands:
        commands[cmd](args)
    else:
        print(f"Unknown command: {cmd}")
        print("Run 'model help' for usage")
        sys.exit(1)


if __name__ == '__main__':
    main()
