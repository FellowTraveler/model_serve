# Model Serving Configuration
# Copy this file to .env and adjust values as needed

# Main llama-swap API port (llama-swap internal)
LLAMA_SWAP_PORT=5847

# Harmony Proxy port (clients should connect here for GPT-OSS support)
# The proxy sits in front of llama-swap and handles Harmony encoding for GPT-OSS
HARMONY_PROXY_PORT=5846

# Harmony models config file (list of models requiring Harmony encoding)
# Default: harmony_models.yaml in same directory as harmony_proxy.py
# HARMONY_MODELS_CONFIG=harmony_models.yaml

# Ollama backend for MXFP4 models (llama.cpp doesn't support MXFP4)
# Models with "mxfp4" in the name are automatically routed here
OLLAMA_BASE=http://localhost:11434

# Base port for llama-server instances (each model gets PORT + offset)
LLAMA_SERVER_BASE_PORT=5850

# Model idle timeout in seconds (30 min = 1800)
MODEL_TTL=1800

# Memory pressure threshold (percentage) for auto-unload
MEMORY_PRESSURE_THRESHOLD=75

# Pressure check interval in seconds
PRESSURE_CHECK_INTERVAL=30

# Minimum idle time (seconds) before a model can be unloaded
# Protects recently-used models from being evicted too quickly
MIN_MODEL_AGE_SECONDS=5

# LM Studio models directory (where Ollama models are symlinked)
MODELS_DIR=~/.cache/lm-studio/models

# Ollama-LM Studio bridge binary path (built from submodule)
# Leave empty to use the bundled submodule (recommended)
BRIDGE_SCRIPT=

# Bridge sync interval in seconds (1 hour = 3600)
BRIDGE_SYNC_INTERVAL=3600

# Model name prefix to distinguish from Ollama models in Open WebUI
# Example: "ms/" makes models appear as "ms/gemma3:12.2b-q8_0"
# ms = model_serve (supports multiple backends: llama-swap, Ollama)
MODEL_PREFIX=ms/

# Default context size for models without custom settings
DEFAULT_CTX_SIZE=8192

# Default number of parallel slots per model
# 1 = single-user (saves memory), 4 = multi-user server
DEFAULT_PARALLEL=1
