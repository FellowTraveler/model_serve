# Model Serving Configuration
# Copy this file to .env and adjust values as needed

# Main llama-swap API port (llama-swap internal)
LLAMA_SWAP_PORT=5847

# Harmony Proxy port (clients should connect here for GPT-OSS support)
# The proxy sits in front of llama-swap and handles Harmony encoding for GPT-OSS
HARMONY_PROXY_PORT=5846

# Base port for llama-server instances (each model gets PORT + offset)
LLAMA_SERVER_BASE_PORT=5850

# Model idle timeout in seconds (30 min = 1800)
MODEL_TTL=1800

# Memory pressure threshold (percentage) for auto-unload
MEMORY_PRESSURE_THRESHOLD=75

# Pressure check interval in seconds
PRESSURE_CHECK_INTERVAL=30

# LM Studio models directory (where Ollama models are symlinked)
MODELS_DIR=~/.cache/lm-studio/models

# Ollama-LM Studio bridge binary path (built from submodule)
# Leave empty to use the bundled submodule (recommended)
BRIDGE_SCRIPT=

# Bridge sync interval in seconds (1 hour = 3600)
BRIDGE_SYNC_INTERVAL=3600

# Model name prefix to distinguish from Ollama models in Open WebUI
# Example: "ls/" makes models appear as "ls/gemma3:12.2b-q8_0"
MODEL_PREFIX=ls/

# Default context size for models without custom settings
DEFAULT_CTX_SIZE=8192

# Default number of parallel slots per model
# 1 = single-user (saves memory), 4 = multi-user server
DEFAULT_PARALLEL=1
