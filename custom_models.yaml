# Custom model settings
# These override the auto-generated defaults from generate_config.py
#
# For each model, you can specify:
#   sampler_args: Additional args appended to llama-server command
#   cmd: Full command override (use ${MODEL_PATH} and ${PORT} placeholders)
#   ttl: Custom idle timeout in seconds
#
# Example entries:

models:
  # Example: Add top-n-sigma sampling to a specific model
  # gemma3:27.4b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"

  # Example: Custom settings for coding model
  # codestral:22.2b-q8_0:
  #   sampler_args: "--top-p 0.9 --temp 0.2"
  #   ttl: 3600  # Keep loaded longer

  # Example: Full command override
  # my-custom-model:
  #   cmd: "llama-server --host 127.0.0.1 --port ${PORT} --model /path/to/model.gguf --ctx-size 4096 --temp 0.8"
