# Custom model settings
# These override the auto-generated defaults from generate_config.py
#
# For each model, you can specify:
#   sampler_args: Additional args appended to llama-server command
#   cmd: Full command override (use ${MODEL_PATH} and ${PORT} placeholders)
#   ttl: Custom idle timeout in seconds
#
# Model names should NOT include the ls/ prefix

models:
  # ============================================================================
  # Top-n-sigma sampling settings
  # Uncomment models you want to use with top-n-sigma sampling
  # ============================================================================

  # --- GPT-OSS Derestricted models ---
  # gpt-oss-120b-derestricted.mxfp4_moe-gguf:117b-unknown:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # gpt-oss-120b-derestricted-gguf:117b-unknown:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # gpt-oss-20b-derestricted-gguf:20.9b-unknown:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # gguf-mxfp4-gpt-oss-20b-derestricted:20.9b-unknown:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"

  # --- Gemma models ---
  # gemma-3-27b-derestricted-gguf:27b-unknown:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # medgemma3:27.0b-q4_k_m:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # translategemma:12.2b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # gemma3:12b-it-qat:  # Not yet available - pull when released
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # gemma3:4b-it-qat:  # Not yet available - pull when released
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"

  # --- GLM models ---
  # glm-4.7-flash-gguf:29.9b-unknown:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # glm-4.7-flash-derestricted-gguf:29.9b-unknown:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"

  # --- Qwen3 models ---
  # qwen3-vl:31.1b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # qwen3-next:79.7b-q4_k_m:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # qwen3:32.8b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # qwen3:32.8b-q4_k_m:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # qwen3:14.8b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # qwen3:8.2b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # qwen3:4.0b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # qwen3:2.0b-q4_k_m:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # qwen3:751.63m-q4_k_m:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"

  # --- Mistral models ---
  # mistral-small3.2:24.0b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # mistral-small3.2:24.0b-q4_k_m:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # ministral-3:13.9b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # ministral-3:8.9b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # ministral-3:3.8b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # devstral-small-2:24.0b-q8_0:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # devstral-small-2:24.0b-q4_k_m:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"

  # --- Llama models ---
  # llama3.2:3.2b-f16:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # llama3.2:1.2b-f16:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"
  # llama3.3:70.6b-q6_k:
  #   sampler_args: "--top-nsigma 1.5 --top-k 0 --top-p 0.95 --temp 0.7"

  # ============================================================================
  # Example: Custom settings for coding model (lower temperature)
  # codestral:22.2b-q8_0:
  #   sampler_args: "--top-p 0.9 --temp 0.2"
  #   ttl: 3600  # Keep loaded longer
  # ============================================================================
